## Introduction
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

This project is a part of Udacity Data Engineering Nanodegree program

## Dataset  Million Song Dataset

#### Song Dataset
Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 

#### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. The log files in the dataset are partitioned by year and month
In order to enable Sparkify to analyze their data, a Relational Database Schema was created, which can be filled with an ETL pipeline.

## Schema for Song Play Analysis
Using the song and log datasets, I created a star schema optimized for queries on song play analysis. This includes the following tables.
#### Fact Table
1. songplays - records in log data associated with song plays i.e. records with page NextSong
   - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
#### Dimension Tables
2. users - users in the app
   - user_id, first_name, last_name, gender, level
3. songs - songs in music database
   - song_id, title, artist_id, year, duration
4. artists - artists in music database
   - artist_id, name, location, latitude, longitude
5. time - timestamps of records in songplays broken down into specific units
   - start_time, hour, day, week, month, year, weekday
   
## Project Template

1. create_tables.py  : drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts
2. etl.ipynb reads and processes a single file from song_data and log_data and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.
3. etl.py reads and processes files from song_data and log_data and loads them into your tables. You can fill this out based on your work in the ETL notebook.
4. sql_queries.py contains all your sql queries, and is imported into the last three files above.

#### The purpose of this database
Using a database makes it easier to analyze the data. By using SQL and the star scheme, joins and aggregations, the data can be searched and summarized quickly and easily. By using a relational database, Sparkify can also perform ad hoc analysis of its database.

## Project Steps
#### Create Tables
1. Wrote CREATE statements in sql_queries.py to create each table.
2. Wrote DROP statements in sql_queries.py to drop each table if it exists.
3. Ran create_tables.py to create database and tables.
4. Ran test.ipynb to confirm the creation of your tables with the correct columns.

#### Built ETL Process
1. Developed ETL Process for each table as in etl.ipynb
2. Ran test.ipynb to confirm that records were successfully inserted into each table

#### Built ETL Pipeline
A. Completed etl.ipynb 
    
1. Performed ETL on the first dataset i.e. song_data to create the songs and artists dimensional tables
   ##### Songs table
   - Extracted data(array and converted to list) from JSON for columns song ID, title, artist ID, year, and duration .
   - Implemented the song_table_insert query in sql_queries.py 
   ##### Artists table
   - Extracted data(array and converted to list) from JSON for columns artist ID, name, location, latitude, and longitude.
   - Implemented artist_table_insert query in sql_queries.
2. Performed ETL on the second dataset, log_data, to create the time and users dimensional tables, as well as the songplays fact table
   ##### Tme table
   - Extracted data(array and converted to list) from JSON for Time Table.
   - Implemented time_table_insert query in sql_queries.
   ##### Users table
   - Extracted data(array and converted to list) from JSON for columns user ID, first name, last name, gender and level 
   - Implemented user_table_insert query in sql_queries
   ##### Tme table
   - information from the songs table, artists table, and original log file are all needed for the songplays table. Since the log file does not specify an ID for either the song      or the artist, I needed to get the song ID and artist ID by querying the songs and artists tables to find matches based on song title, artist name, and song duration time.
   - Implemented the song_select query in sql_queries.py to find the song ID and artist ID based on the title, artist name, and duration of a song.
   - Selected the timestamp, user ID, level, song ID, artist ID, session ID, location, and user agent and set to songplay_data
   - Implemented songplay_table_insert query in sql_queries
 
 B. Used etl.ipynb to complete etl.py  

## Summary
In order to enable Sparkify to analyze their data, a Relational Database Schema was created. Data is later loaded into the tables via an ETL pipeline.

The star schema enables the company to view the user behaviour over several dimensions. The fact table is used to store all user song activities that contain the category "NextSong". Using this table, the company can relate and analyze the dimensions users, songs, artists and time.


